{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8638a99c",
   "metadata": {},
   "source": [
    "# Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba029c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "SEED = 187 # set seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b4b763",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933651c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "#  PARAMETERS \n",
    "# --------------------------------------------------\n",
    "PRICE = 30.0            # revenue per unit sold\n",
    "HOLDING_COST = 8.0      # cost per unit held one extra period\n",
    "SCRAP_COST = 8.0        # cost per unit scrapped immediately\n",
    "SHORTAGE_COST = 13.0     # penalty per unit unmet demand\n",
    "\n",
    "# Number of batches (optional)\n",
    "NUM_BATCHES = 1\n",
    "\n",
    "INITIAL_INVENTORY = 100  # starting units per Batch\n",
    "HORIZON = 50           # number of decision periods per episode\n",
    "NUM_EPISODES = 50      # Monte Carlo repetitions\n",
    "VECTOR_DIM = 2          # feature vector = [1, scaled_age]\n",
    "\n",
    "# FRACTION (arms) f_t\n",
    "SCRAP_FRACTIONS = np.linspace(0.0, 1.0, 99)  # can be adjusted \n",
    "\n",
    "\n",
    "# DGLM PARAMETERS (specified in emprircal setup, uncomment otherwise)\n",
    "# --------------------------------------------------\n",
    "# process_noises = [   \n",
    "#     0.01,  # Batch 1\n",
    "#     0.01,  # Batch 2\n",
    "#     0.01,  # Batch 3\n",
    "# ]\n",
    "\n",
    "# For plotting\n",
    "# --------------------------------------------------\n",
    "t_min = 5.0  # minimum x axis\n",
    "t_max = 15.0  # maximum x axis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c7c224",
   "metadata": {},
   "source": [
    "# Demand Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa3538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMAND\n",
    "# Characteristics of the demand process\n",
    "demand_specs = [\n",
    "    {\"batch\": 1, \"intercept\": np.log(30),  \"age_slope\": -0.0},   # Seems to struggle with high demand numbers\n",
    "    {\"batch\": 2, \"intercept\": np.log(30),  \"age_slope\": -0.0},  \n",
    "    {\"batch\": 3, \"intercept\": np.log(30),  \"age_slope\": -0.0},\n",
    "]\n",
    "df_specs = pd.DataFrame(demand_specs)\n",
    "\n",
    "# Store true betas for each batch\n",
    "true_betas = df_specs[[\"intercept\", \"age_slope\"]].values \n",
    "\n",
    "# Build true lambdas\n",
    "true_lams_ex = np.zeros((NUM_BATCHES, HORIZON+1))\n",
    "for b in range(NUM_BATCHES):\n",
    "    for t in range(HORIZON+1):\n",
    "        x = np.array([1.0, t / HORIZON])\n",
    "        true_lams_ex[b, t] = np.exp(true_betas[b] @ x)\n",
    "df_true_lams = pd.DataFrame(true_lams_ex)\n",
    "df_true_lams.index.name = \"Batch\"\n",
    "\n",
    "\n",
    "# Inspect\n",
    "print(\"Demand Specs\")\n",
    "display(df_specs)\n",
    "print(\"\\nTrue Lambda Paths\")\n",
    "display(df_true_lams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55453cd7",
   "metadata": {},
   "source": [
    "## Experiment Settings Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPISODES\n",
    "num_grid_episodes = 50\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "decay_rates = [-0.1, -2.5, -5.0]\n",
    "process_noises = [0.01]\n",
    "min_scrap_ages = [float(i) for i in range(1, 41)]  # minimum ages of scrap from 1 to 40\n",
    "\n",
    "# For storing custom priors\n",
    "custom_prior_means = {}\n",
    "custom_prior_covs = {}\n",
    "\n",
    "# Initialize with grid-specific priors\n",
    "for decay_rate in decay_rates:\n",
    "    for noise in process_noises:\n",
    "        for scrap_age in min_scrap_ages:\n",
    "            key = (decay_rate, noise, scrap_age)\n",
    "            \n",
    "            # Mean\n",
    "            custom_prior_means[key] = [\n",
    "                np.array([np.log(30), 0]),  # Batch 1 (intercept, decay rate)\n",
    "                np.array([np.log(30), 0]),  # Batch 2 (intercept, decay rate)\n",
    "                np.array([np.log(30), 0]),  # Batch 3 (intercept, decay rate)\n",
    "            ]\n",
    "            \n",
    "            # Covariance\n",
    "            base_var = 1  # Base variance\n",
    "            cov_scale = max(1.0, 0)  # Scale factor based on process noise\n",
    "            custom_prior_covs[key] = base_var * cov_scale * np.eye(2)\n",
    "\n",
    "# Initialize dictionary to store results\n",
    "grid_results = {}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36547bc",
   "metadata": {},
   "source": [
    "# Economic Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "#  FOR EXPECTED SALES & PROFIT\n",
    "# --------------------------------------------------\n",
    "def expected_sales(k: int, lam: float) -> float:\n",
    "    if lam <= 0:\n",
    "        return 0.0\n",
    "    lam_cap = min(lam, 1000.0) # caped to avoid overflow\n",
    "    sales = sum(n * stats.poisson.pmf(n, lam_cap) for n in range(k + 1))\n",
    "    tail = stats.poisson.sf(k, lam_cap)\n",
    "    return sales + k * tail\n",
    "\n",
    "def expected_future_profit(hold: int, lam: float) -> float:\n",
    "    lam = float(np.clip(lam, 0.01, 1000.0))\n",
    "    s = expected_sales(hold, lam)\n",
    "    leftover = hold - s\n",
    "    shortage = lam - s\n",
    "    return s * PRICE - leftover * HOLDING_COST - shortage * SHORTAGE_COST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7892293b",
   "metadata": {},
   "source": [
    "# Poisson-GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fdb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "#  BAYESIAN POISSON DGLM (safeguarded against overflow)\n",
    "# --------------------------------------------------\n",
    "class BayesianPoissonDGLM:\n",
    "    def __init__(self, dim: int, prior_var: float, process_noise: float, prior_mean=None):\n",
    "        self.d = dim\n",
    "        \n",
    "        self.mu = prior_mean if prior_mean is not None else np.zeros(dim)\n",
    "        \n",
    "        if not isinstance(self.mu, np.ndarray):\n",
    "            self.mu = np.array([self.mu] * dim if np.isscalar(self.mu) else self.mu)\n",
    "            \n",
    "        # Validate\n",
    "        if len(self.mu) != dim:\n",
    "            raise ValueError(f\"Prior mean dim({len(self.mu)}) model dim ({dim})\")\n",
    "            \n",
    "        if isinstance(prior_var, (int, float)):\n",
    "            self.Sigma = prior_var * np.eye(dim)\n",
    "        else:\n",
    "            self.Sigma = prior_var\n",
    "            \n",
    "        self.W = process_noise * np.eye(dim)\n",
    "        \n",
    "\n",
    "    # Thompson sampling\n",
    "    def sample_posterior(self, seed=None) -> np.ndarray: \n",
    "        # Local RNG for reproducibility\n",
    "        if seed is not None:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            return rng.multivariate_normal(self.mu, self.Sigma)\n",
    "        else:\n",
    "            return np.random.multivariate_normal(self.mu, self.Sigma)\n",
    "\n",
    "    # Prediction of mean demand rate\n",
    "    def predict_mean(self, x: np.ndarray, beta: np.ndarray = None) -> float:\n",
    "        b = beta if beta is not None else self.mu\n",
    "        eta = float(b @ x)\n",
    "        eta = np.clip(eta, -1000.0, 1000.0)\n",
    "        return float(np.exp(eta))\n",
    "\n",
    "    # Update the model with new observation\n",
    "    def update(self, x: np.ndarray, y: int) -> None:\n",
    "        mu_p = self.mu.copy() \n",
    "        Sigma_p = self.Sigma + self.W # process noise !!\n",
    "\n",
    "        eta_p = float(mu_p @ x)\n",
    "        eta_p = np.clip(eta_p, -1000.0, 1000.0)\n",
    "        lam_p = max(np.exp(eta_p), 1e-8)\n",
    "\n",
    "        z = eta_p + (y - lam_p) / lam_p\n",
    "        R = 1.0 / lam_p\n",
    "        R = max(R, 0.001) # safeguard against division by zero\n",
    "\n",
    "        H = x.reshape(1, -1)\n",
    "        S = float((H @ Sigma_p @ H.T + R)[0][0])\n",
    "        S = max(S, 1e-8)\n",
    "        K = (Sigma_p @ H.T).flatten() / S\n",
    "\n",
    "        self.mu = mu_p + K * (z - eta_p)\n",
    "        self.mu = np.clip(self.mu, -1000.0, 1000.0)\n",
    "\n",
    "        IKH = np.eye(self.d) - np.outer(K, H.flatten())\n",
    "        self.Sigma = IKH @ Sigma_p @ IKH.T + 1e-6 * np.eye(self.d)\n",
    "        eigs = np.linalg.eigvalsh(self.Sigma)\n",
    "        if np.min(eigs) <= 0:\n",
    "            self.Sigma += (abs(np.min(eigs)) + 1e-6) * np.eye(self.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcecc37b",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "#  SIMULATION OF ONE EPISODE\n",
    "# --------------------------------------------------\n",
    "def run_episode(seed=None, prior_means=None, prior_cov=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)    # Sample each batch's initial demand rate path\n",
    "\n",
    "    # Use default parameters if not provided\n",
    "    if prior_means is None:\n",
    "        # Create a default prior with zeros or appropriate values\n",
    "        prior_means = [np.array([0.0, 0.0]) for _ in range(NUM_BATCHES)]\n",
    "\n",
    "    if prior_cov is None:\n",
    "        # Use a default covariance, perhaps 1.0 * identity matrix\n",
    "        prior_cov = 1.0\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # DEMAND GENERATION\n",
    "\n",
    "    true_betas = df_specs[[\"intercept\", \"age_slope\"]].values\n",
    "    true_lams  = np.zeros((NUM_BATCHES, HORIZON+1))\n",
    "    for b in range(NUM_BATCHES):\n",
    "        for t in range(HORIZON+1):\n",
    "            x = np.array([1.0, t / HORIZON])\n",
    "            true_lams[b, t] = np.exp(true_betas[b] @ x)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "    # Initialize inventory and models\n",
    "    inventory = np.full(NUM_BATCHES, INITIAL_INVENTORY, dtype=int)\n",
    "    models = []\n",
    "    for b in range(NUM_BATCHES):\n",
    "        if isinstance(prior_cov, np.ndarray) and prior_cov.ndim == 3:\n",
    "            batch_cov = prior_cov[b]  # Extract just this batch's covariance\n",
    "        else:\n",
    "            batch_cov = prior_cov\n",
    "            \n",
    "        models.append(BayesianPoissonDGLM(\n",
    "            VECTOR_DIM,\n",
    "            batch_cov,\n",
    "            process_noises[b],\n",
    "            prior_mean=prior_means[b]\n",
    "        ))\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    #  ALLOCATE TRACKERS\n",
    "    # --------------------------------------------------\n",
    "    inv_pre    = np.zeros((NUM_BATCHES, HORIZON))\n",
    "    dem_obs    = np.zeros((NUM_BATCHES, HORIZON))\n",
    "    sales      = np.zeros((NUM_BATCHES, HORIZON))\n",
    "    scr_frac   = np.zeros((NUM_BATCHES, HORIZON))\n",
    "    oracle_f   = np.zeros((NUM_BATCHES, HORIZON))\n",
    "    inv_post   = np.zeros((NUM_BATCHES, HORIZON))\n",
    "    beta_draws = np.zeros((NUM_BATCHES, HORIZON, VECTOR_DIM))\n",
    "    lam_hats   = np.zeros((NUM_BATCHES, HORIZON))\n",
    "    mu_history = np.zeros((NUM_BATCHES, HORIZON+1, VECTOR_DIM))\n",
    "    Sigma_history = np.zeros((NUM_BATCHES, HORIZON+1, VECTOR_DIM, VECTOR_DIM))\n",
    "    \n",
    "    \n",
    "    # record initial prior\n",
    "    for b in range(NUM_BATCHES):\n",
    "        mu_history[b, 0, :] = models[b].mu\n",
    "        Sigma_history[b, 0, :, :] = models[b].Sigma\n",
    "\n",
    "\n",
    "\n",
    "    preds   = np.zeros((NUM_BATCHES, HORIZON))\n",
    "    regrets = np.zeros((NUM_BATCHES, HORIZON))\n",
    "\n",
    "    # to avoid indexing issues\n",
    "    h = true_lams.shape[1] - 1  # number of decision periods\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    #  MONTHLY LOOP\n",
    "    # --------------------------------------------------\n",
    "    for t in range(h):\n",
    "        # 1) Record inventory at period start\n",
    "        inv_pre[:, t] = inventory.copy()\n",
    "\n",
    "        # Observe demands (still generate demands even during no-sales periods)\n",
    "        demands = np.random.poisson(true_lams[:, t])\n",
    "\n",
    "        # Demand learning phase\n",
    "        if t < min_scrap_age:\n",
    "            dem_obs[:, t] = demands  # Still observe demand for learning\n",
    "            sales[:, t] = 0          # But record zero sales\n",
    "            \n",
    "            # Update model beliefs based on observed demand\n",
    "            for b in range(NUM_BATCHES):\n",
    "                x = np.array([1.0, t / HORIZON])\n",
    "                models[b].update(x, demands[b])\n",
    "                mu_history[b, t+1, :] = models[b].mu\n",
    "                Sigma_history[b, t+1, :, :] = models[b].Sigma\n",
    "                \n",
    "\n",
    "                next_x = np.array([1.0, (t + 1) / HORIZON])\n",
    "                preds[b, t] = models[b].predict_mean(next_x)\n",
    "            \n",
    "                regrets[b, t] = 0\n",
    "                scr_frac[b, t] = 0\n",
    "                oracle_f[b, t] = 0\n",
    "                inv_post[b, t] = inventory[b]  # keep inventory to determine overstock directly\n",
    "\n",
    "            # Skip the rest of the loop logic\n",
    "            continue\n",
    "        \n",
    "        # Observe demands\n",
    "        demands = np.random.poisson(true_lams[:, t])\n",
    "        dem_obs[:, t] = demands\n",
    "\n",
    "        for b in range(NUM_BATCHES):\n",
    "            x = np.array([1.0, t / HORIZON])\n",
    "            models[b].update(x, demands[b])\n",
    "            mu_history[b, t+1, :] = models[b].mu\n",
    "            Sigma_history[b, t+1, :, :] = models[b].Sigma\n",
    "\n",
    "        # 3) Thompson sample one beta per batch\n",
    "        beta_samps = [models[b].sample_posterior(seed=SEED + t * NUM_BATCHES + b) for b in range(NUM_BATCHES)]\n",
    "\n",
    "        # 4) For each batch decide scrap fraction and record all metrics\n",
    "        for b in range(NUM_BATCHES):\n",
    "            # Actual sales\n",
    "            sold = min(inventory[b], demands[b])\n",
    "            sales[b, t] = sold\n",
    "\n",
    "            # Revenue, shortage, leftover\n",
    "            shortage = max(demands[b] - inventory[b], 0)\n",
    "            revenue  = sold * PRICE\n",
    "            leftover = inventory[b] - sold\n",
    "            base     = revenue - shortage * SHORTAGE_COST\n",
    "\n",
    "            # Next‐period context\n",
    "            next_x = np.array([1.0, (t + 1) / HORIZON])\n",
    "\n",
    "            # Track the sampled beta and the predicted lambda\n",
    "            beta_draws[b, t, :] = beta_samps[b]\n",
    "            lam_hats[b,    t]   = models[b].predict_mean(next_x, beta_samps[b])\n",
    "\n",
    "            # Greedy evaluation over scrap fractions\n",
    "            best_val = -math.inf\n",
    "            best_f   = 0.0\n",
    "\n",
    "            # Determine available scrap fractions\n",
    "            available_fractions = [0.0] if t < min_scrap_age else SCRAP_FRACTIONS\n",
    "\n",
    "            for f in available_fractions:\n",
    "                hold = leftover - round(f * leftover)\n",
    "                sc   = (leftover - hold) * SCRAP_COST\n",
    "                hc   = hold * HOLDING_COST\n",
    "\n",
    "                lam_hat = models[b].predict_mean(next_x, beta_samps[b])\n",
    "                fut     = expected_future_profit(hold, lam_hat)\n",
    "                val     = base - sc - hc + fut\n",
    "                if val > best_val:\n",
    "                    best_val = val\n",
    "                    best_f   = f\n",
    "\n",
    "            # Record chosen scrap fraction\n",
    "            scr_frac[b, t] = best_f\n",
    "\n",
    "            # Apply action\n",
    "            hold = leftover - round(best_f * leftover)\n",
    "            preds[b, t] = models[b].predict_mean(next_x)  # posterior‐mean forecast\n",
    "\n",
    "            # 5) Compute regret\n",
    "            lam_next = true_lams[b, t+1]\n",
    "            agent_val = base \\\n",
    "                        - ((leftover - hold) * SCRAP_COST + hold * HOLDING_COST) \\\n",
    "                        + expected_future_profit(hold, lam_next)\n",
    "\n",
    "            oracle_vals = []\n",
    "            for f in available_fractions:\n",
    "                h2   = leftover - round(f * leftover)\n",
    "                sc2  = (leftover - h2) * SCRAP_COST\n",
    "                hc2  = h2 * HOLDING_COST\n",
    "                fut2 = expected_future_profit(h2, lam_next)\n",
    "                oracle_vals.append(base - sc2 - hc2 + fut2)\n",
    "\n",
    "            # Record oracle's scrap fraction and regret\n",
    "            best_oracle_f    = SCRAP_FRACTIONS[np.argmax(oracle_vals)]\n",
    "            oracle_f[b, t]   = best_oracle_f\n",
    "            regrets[b, t]    = max(oracle_vals) - agent_val\n",
    "\n",
    "            # 6) Update inventory and track post‐period level\n",
    "            inventory[b]     = hold\n",
    "            inv_post[b, t]   = hold\n",
    "\n",
    "    return {\n",
    "        \"true_lams\":  true_lams,\n",
    "        \"preds\":      preds,\n",
    "        \"regrets\":    regrets,\n",
    "        \"inv_pre\":    inv_pre,\n",
    "        \"dem_obs\":    dem_obs,\n",
    "        \"sales\":      sales,\n",
    "        \"scr_frac\":   scr_frac,\n",
    "        \"oracle_f\":   oracle_f,\n",
    "        \"inv_post\":   inv_post,\n",
    "        \"beta_draws\": beta_draws,\n",
    "        \"lam_hats\":   lam_hats,\n",
    "        \"mu_history\": mu_history,\n",
    "        \"Sigma_history\": Sigma_history\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b2191",
   "metadata": {},
   "source": [
    "# RUN EXPERIMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365dd5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_scrap_age = 1.0 \n",
    "\n",
    "# Grid search across all parameter combinations\n",
    "for decay_rate in decay_rates:\n",
    "    for noise in process_noises:\n",
    "        for scrap_age in min_scrap_ages:\n",
    "            key = (decay_rate, noise, scrap_age)\n",
    "            print(f\"\\nRunning simulations for decay={decay_rate}, noise={noise}, min_scrap_age={scrap_age}\")\n",
    "            \n",
    "            # Save original parameters\n",
    "            original_df_specs = df_specs.copy()\n",
    "            original_process_noises = process_noises.copy()\n",
    "            original_min_scrap_age = min_scrap_age  # Store original value\n",
    "            \n",
    "            # Update parameters for this run\n",
    "            updated_demand_specs = [\n",
    "                {\"batch\": 1, \"intercept\": np.log(30), \"age_slope\": decay_rate},\n",
    "                {\"batch\": 2, \"intercept\": np.log(30), \"age_slope\": decay_rate},\n",
    "                {\"batch\": 3, \"intercept\": np.log(30), \"age_slope\": decay_rate}\n",
    "            ]\n",
    "            \n",
    "            # Set parameters for this grid cell\n",
    "            df_specs = pd.DataFrame(updated_demand_specs)\n",
    "            true_betas = df_specs[[\"intercept\", \"age_slope\"]].values\n",
    "            process_noises = [noise] * NUM_BATCHES\n",
    "            min_scrap_age = scrap_age \n",
    "            \n",
    "            # Update true lambda values\n",
    "            true_lams_ex = np.zeros((NUM_BATCHES, HORIZON+1))\n",
    "            for b in range(NUM_BATCHES):\n",
    "                for t in range(HORIZON+1):\n",
    "                    x = np.array([1.0, t / HORIZON])\n",
    "                    true_lams_ex[b, t] = np.exp(true_betas[b] @ x)\n",
    "            \n",
    "            # Run episodes for this parameter combination\n",
    "            ep_results = []\n",
    "            regrets_sum = np.zeros((NUM_BATCHES, HORIZON))\n",
    "            rmse_sum = np.zeros(HORIZON)\n",
    "            mae_sum = np.zeros(HORIZON)\n",
    "            \n",
    "            for ep in range(num_grid_episodes):\n",
    "                result = run_episode(\n",
    "                    seed=SEED + ep,  # Use different seed for each episode\n",
    "                    prior_means=custom_prior_means[key],  # Prior mean\n",
    "                    prior_cov=custom_prior_covs[key]      # Covariance\n",
    "                )\n",
    "                \n",
    "                # Store result and calculate metrics\n",
    "                ep_results.append(result)\n",
    "                regrets_sum += result[\"regrets\"]\n",
    "                \n",
    "                # Calculate RMSE and MAE\n",
    "                true_lams = result[\"true_lams\"][:, 1:]  # drop t=0\n",
    "                preds = result[\"preds\"]\n",
    "                rmse = np.sqrt(((true_lams - preds) ** 2).mean(axis=0))\n",
    "                mae = np.abs(true_lams - preds).mean(axis=0)\n",
    "                rmse_sum += rmse\n",
    "                mae_sum += mae\n",
    "            \n",
    "            # Calculate average metrics\n",
    "            avg_regrets = regrets_sum / num_grid_episodes\n",
    "            avg_rmse = rmse_sum / num_grid_episodes\n",
    "            avg_mae = mae_sum / num_grid_episodes\n",
    "            \n",
    "            # Store results for THIS specific combination\n",
    "            grid_results[key] = {\n",
    "                \"decay_rate\": decay_rate,\n",
    "                \"process_noise\": noise,\n",
    "                \"min_scrap_age\": scrap_age,\n",
    "                \"avg_regret\": avg_regrets.mean(),\n",
    "                \"cum_regret\": avg_regrets.sum(),\n",
    "                \"avg_rmse\": avg_rmse.mean(),\n",
    "                \"avg_mae\": avg_mae.mean(),\n",
    "                \"regret_trajectory\": avg_regrets.mean(axis=0),\n",
    "                \"rmse_trajectory\": avg_rmse,\n",
    "                \"mae_trajectory\": avg_mae,\n",
    "                \"episodes\": ep_results\n",
    "            }\n",
    "            \n",
    "            # Report key metrics\n",
    "            print(f\"  Average regret: {avg_regrets.mean():.4f}\")\n",
    "            print(f\"  Cumulative regret: {avg_regrets.sum():.4f}\")\n",
    "            print(f\"  Average RMSE: {avg_rmse.mean():.4f}\")\n",
    "            print(f\"  Average MAE: {avg_mae.mean():.4f}\")\n",
    "            \n",
    "            # Restore original parameters\n",
    "            df_specs = original_df_specs\n",
    "            process_noises = original_process_noises\n",
    "            min_scrap_age = original_min_scrap_age  # Restore original value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce9ed0",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be2b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create scrapping analysis tables\n",
    "def create_scrapping_tables(grid_results, decay_rates, min_scrap_ages, process_noise=0.01):\n",
    "    tables = {}\n",
    "    \n",
    "    for decay_rate in decay_rates:\n",
    "        for scrap_age in min_scrap_ages:\n",
    "            key = (decay_rate, process_noise, scrap_age)\n",
    "            if key not in grid_results:\n",
    "                print(f\"Missing data for {key}\")\n",
    "                continue\n",
    "                \n",
    "            episodes = grid_results[key][\"episodes\"]\n",
    "            \n",
    "\n",
    "            oracle_fractions = []\n",
    "            agent_fractions = []\n",
    "            cumulative_regrets = []\n",
    "            units_scrapped = []\n",
    "            inv_pres = []\n",
    "            sales_list = []\n",
    "            calculated_scrap = []\n",
    "            \n",
    "\n",
    "            for ep in episodes:\n",
    "\n",
    "                oracle_f = np.mean(ep[\"oracle_f\"], axis=0)\n",
    "                agent_f = np.mean(ep[\"scr_frac\"], axis=0)\n",
    "                regrets = np.mean(ep[\"regrets\"], axis=0)\n",
    "                cum_regret = np.cumsum(regrets)\n",
    "                \n",
    "                inv_pre = np.mean(ep[\"inv_pre\"], axis=0)\n",
    "                inv_post = np.mean(ep[\"inv_post\"], axis=0)\n",
    "                ep_sales = np.mean(ep[\"sales\"], axis=0)\n",
    "                \n",
    "                scrapped = inv_pre - inv_post\n",
    "                calc_scrap = (inv_pre - ep_sales) * agent_f\n",
    "                \n",
    "                oracle_fractions.append(oracle_f)\n",
    "                agent_fractions.append(agent_f)\n",
    "                cumulative_regrets.append(cum_regret)\n",
    "                units_scrapped.append(scrapped)\n",
    "                inv_pres.append(inv_pre)\n",
    "                sales_list.append(ep_sales)\n",
    "                calculated_scrap.append(calc_scrap)\n",
    "            \n",
    "            # Average across episodes\n",
    "            avg_oracle = np.mean(oracle_fractions, axis=0)\n",
    "            avg_agent = np.mean(agent_fractions, axis=0)\n",
    "            avg_cum_regret = np.mean(cumulative_regrets, axis=0)\n",
    "            avg_units_scrapped = np.mean(units_scrapped, axis=0)\n",
    "            avg_inv_pre = np.mean(inv_pres, axis=0)\n",
    "            avg_sales = np.mean(sales_list, axis=0)\n",
    "            avg_calc_scrap = np.mean(calculated_scrap, axis=0)\n",
    "            \n",
    "            time_steps = np.arange(len(avg_oracle))\n",
    "            \n",
    "            start_idx = int(scrap_age)\n",
    "            relevant_steps = range(start_idx, min(start_idx + 10, len(time_steps)))\n",
    "            \n",
    "            data = {\n",
    "                'Time Step': time_steps[relevant_steps],\n",
    "                'Oracle Scrap Fraction': np.round(avg_oracle[relevant_steps], 2),\n",
    "                'Agent Scrap Fraction': np.round(avg_agent[relevant_steps], 2),\n",
    "                'Cumulative Regret': np.round(avg_cum_regret[relevant_steps], 2),\n",
    "                'Inventory Before': np.round(avg_inv_pre[relevant_steps], 2),\n",
    "                'Sales': np.round(avg_sales[relevant_steps], 2),\n",
    "                'Calculated Units Scrapped': np.round(avg_calc_scrap[relevant_steps], 2)\n",
    "            }\n",
    "            \n",
    "            tables[(decay_rate, scrap_age)] = pd.DataFrame(data)\n",
    "            \n",
    "    return tables\n",
    "\n",
    "\n",
    "scrapping_tables = create_scrapping_tables(\n",
    "    grid_results, \n",
    "    decay_rates=[-0.1, -2.5, -5.0], \n",
    "    min_scrap_ages=[10.0, 20.0, 40.0]\n",
    ")\n",
    "\n",
    "for (decay, scrap_age), table in scrapping_tables.items():\n",
    "    print(f\"\\n=== Decay Rate: {decay}, Min Scrap Age: {scrap_age} ===\")\n",
    "    display(table)\n",
    "\n",
    "    with open(\"scrapping_tables_100_1-40.txt\", \"w\") as f:\n",
    "        for (decay, scrap_age), table in scrapping_tables.items():\n",
    "            f.write(f\"=== Decay Rate: {decay}, Min Scrap Age: {scrap_age} ===\\n\")\n",
    "            f.write(table.to_string())\n",
    "            f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "#  PLOTTING FIRST SCRAP FRACTIONS\n",
    "# --------------------------------------------------\n",
    "first_scrap_data = []\n",
    "for decay_rate in decay_rates:\n",
    "    for scrap_age in min_scrap_ages:\n",
    "        key = (decay_rate, process_noises[0], scrap_age)\n",
    "        episodes = grid_results[key][\"episodes\"]\n",
    "        oracle_first_scraps = []\n",
    "        agent_first_scraps = []\n",
    "        for ep in episodes:\n",
    "            scrap_start_idx = int(scrap_age)\n",
    "            oracle_first_scrap = np.mean(ep[\"oracle_f\"][:, scrap_start_idx])\n",
    "            agent_first_scrap = np.mean(ep[\"scr_frac\"][:, scrap_start_idx])\n",
    "            oracle_first_scraps.append(oracle_first_scrap)\n",
    "            agent_first_scraps.append(agent_first_scrap)\n",
    "        avg_oracle_first = np.mean(oracle_first_scraps)\n",
    "        avg_agent_first = np.mean(agent_first_scraps)\n",
    "        first_scrap_data.append({\n",
    "            \"Decay Rate\": decay_rate,\n",
    "            \"Min Scrap Age\": scrap_age,\n",
    "            \"Decision Maker\": \"Oracle\",\n",
    "            \"First Scrap Fraction\": avg_oracle_first\n",
    "        })\n",
    "        first_scrap_data.append({\n",
    "            \"Decay Rate\": decay_rate,\n",
    "            \"Min Scrap Age\": scrap_age,\n",
    "            \"Decision Maker\": \"Agent\",\n",
    "            \"First Scrap Fraction\": avg_agent_first\n",
    "        })\n",
    "\n",
    "first_scrap_df = pd.DataFrame(first_scrap_data)\n",
    "positions = np.arange(len(min_scrap_ages))\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "width = 0.15\n",
    "\n",
    "for i, decay_rate in enumerate(decay_rates):\n",
    "    subset = first_scrap_df[first_scrap_df[\"Decay Rate\"] == decay_rate]\n",
    "    oracle_vals = []\n",
    "    agent_vals = []\n",
    "    for age in min_scrap_ages:\n",
    "        age_subset = subset[subset[\"Min Scrap Age\"] == age]\n",
    "        oracle_vals.append(age_subset[age_subset[\"Decision Maker\"] == \"Oracle\"][\"First Scrap Fraction\"].values[0])\n",
    "        agent_vals.append(age_subset[age_subset[\"Decision Maker\"] == \"Agent\"][\"First Scrap Fraction\"].values[0])\n",
    "    plt.bar(positions + (i*2-1)*width, oracle_vals, width=width, \n",
    "            label=f\"Oracle (Decay={decay_rate})\", \n",
    "            color=plt.cm.viridis(i/len(decay_rates)), alpha=0.8)\n",
    "    plt.bar(positions + (i*2)*width, agent_vals, width=width, \n",
    "            label=f\"Bandit (Decay={decay_rate})\", \n",
    "            color=plt.cm.viridis(i/len(decay_rates)), alpha=0.5, hatch=\"//\")\n",
    "\n",
    "plt.xticks(positions, [f\"{int(a)}\" for a in min_scrap_ages], fontsize=12)\n",
    "plt.xlabel('Age at First Scrap', fontsize=14)\n",
    "plt.ylabel('First Scrap Fraction', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(fontsize=10, loc='best')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"first_scrap_fractions_100_1-40_1.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6f93fd",
   "metadata": {},
   "source": [
    "# Generating new stats\n",
    "\n",
    "- \"run_episode\" function gnereates experiment data\n",
    "- grid_results stores the results of the different parameters tested\n",
    "- \"return\" command shows, where to allocate metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
